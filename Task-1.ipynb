{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3164b1f8-a6a4-49a9-91f4-f381bf3653c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.models import vgg16\n",
    "import torch.nn.functional as F\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2c59dff-e908-4f95-a926-b88bc9b42914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "261d1192-3be3-4423-b712-898058438897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabColorTransform:\n",
    "    def __call__(self, img):\n",
    "        img_np = np.array(img) / 255.0\n",
    "        lab_img = rgb2lab(img_np).astype(\"float32\")\n",
    "        L = lab_img[:, :, 0] / 100.0  # Normalize L to [0, 1]\n",
    "        ab = lab_img[:, :, 1:] / 128.0  # Normalize ab to [-1, 1]\n",
    "        return torch.tensor(L).unsqueeze(0), torch.tensor(ab).permute(2, 0, 1)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    LabColorTransform()\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 (or your custom dataset)\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92c01ad6-508e-4ccc-90b4-4eb3671a90b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()  # Output in [-1, 1] range for ab channels\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eeb42318-672d-4f2b-a523-5c591840a603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohanty Sameer\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mohanty Sameer\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, vgg):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        # MSE Loss on ab channels\n",
    "        mse_loss = self.mse(output, target)\n",
    "        \n",
    "        # Perceptual Loss\n",
    "        if output.shape[1] == 2:  # Convert to 3-channel for VGG\n",
    "            output_rgb = torch.cat([torch.zeros_like(output[:, :1]), output], dim=1)\n",
    "            target_rgb = torch.cat([torch.zeros_like(target[:, :1]), target], dim=1)\n",
    "        \n",
    "        output_rgb = F.interpolate(output_rgb, size=(224, 224), mode='bilinear')\n",
    "        target_rgb = F.interpolate(target_rgb, size=(224, 224), mode='bilinear')\n",
    "        output_features = self.vgg(output_rgb)\n",
    "        target_features = self.vgg(target_rgb)\n",
    "        perceptual_loss = self.mse(output_features, target_features)\n",
    "        \n",
    "        return mse_loss + 0.1 * perceptual_loss\n",
    "\n",
    "# Initialize VGG for perceptual loss\n",
    "vgg_model = vgg16(pretrained=True).features[:9].eval().to(device)\n",
    "for param in vgg_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1260fb9f-0a01-459c-aa86-9d8ff4ea61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ColorizationNet().to(device)\n",
    "criterion = CombinedLoss(vgg_model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4f4d2ae-10bb-41ad-b419-08f66278bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for L, ab in train_loader:\n",
    "            L, ab = L.to(device), ab.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(L)\n",
    "            loss = criterion(outputs, ab)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        # Save model checkpoint\n",
    "        torch.save(model.state_dict(), f\"colorization_model_epoch{epoch+1}.pth\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    torch.save(model.state_dict(), \"final_colorization_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fb18b8b-28f1-4c94-91ef-5be148de64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_psnr = 0\n",
    "    total_ssim = 0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for L, ab in dataloader:\n",
    "            L, ab = L.to(device), ab.to(device)\n",
    "            outputs = model(L)\n",
    "            \n",
    "            # Convert to numpy for evaluation\n",
    "            outputs_np = outputs.cpu().numpy()\n",
    "            ab_np = ab.cpu().numpy()\n",
    "            L_np = L.cpu().numpy()\n",
    "            \n",
    "            # Calculate metrics for each image in batch\n",
    "            for i in range(outputs_np.shape[0]):\n",
    "                # Reconstruct LAB image\n",
    "                pred_lab = np.concatenate([\n",
    "                    (L_np[i][0] * 100).astype('float32'),\n",
    "                    (outputs_np[i] * 128).astype('float32')\n",
    "                ], axis=0).transpose(1, 2, 0)\n",
    "                \n",
    "                target_lab = np.concatenate([\n",
    "                    (L_np[i][0] * 100).astype('float32'),\n",
    "                    (ab_np[i] * 128).astype('float32')\n",
    "                ], axis=0).transpose(1, 2, 0)\n",
    "                \n",
    "                # Convert to RGB for visualization\n",
    "                pred_rgb = lab2rgb(pred_lab)\n",
    "                target_rgb = lab2rgb(target_lab)\n",
    "                # Calculate metrics\n",
    "                total_psnr += psnr(target_rgb, pred_rgb, data_range=1.0)\n",
    "                total_ssim += ssim(target_rgb, pred_rgb, multichannel=True, data_range=1.0)\n",
    "                count += 1\n",
    "                \n",
    "    print(f\"Average PSNR: {total_psnr/count:.4f}\")\n",
    "    print(f\"Average SSIM: {total_ssim/count:.4f}\")\n",
    "    return total_psnr/count, total_ssim/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cdf0c0-0bc9-4603-aa15-431d15058f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def color_confusion_matrix(model, dataloader):\n",
    "    model.eval()\n",
    "    true_colors = []\n",
    "    pred_colors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for L, ab in dataloader:\n",
    "            L, ab = L.to(device), ab.to(device)\n",
    "            outputs = model(L)\n",
    "            \n",
    "            # Convert to color bins (8 bins per channel)\n",
    "            true_bins = (ab * 4 + 4).clamp(0, 7).long().flatten()\n",
    "            pred_bins = (outputs * 4 + 4).clamp(0, 7).long().flatten()\n",
    "            \n",
    "            true_colors.extend(true_bins.cpu().numpy())\n",
    "            pred_colors.extend(pred_bins.cpu().numpy())\n",
    "    \n",
    "    cm = confusion_matrix(true_colors, pred_colors, labels=range(8))\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Color Value Confusion Matrix')\n",
    "    plt.xlabel('Predicted Color Bins')\n",
    "    plt.ylabel('True Color Bins')\n",
    "    plt.show()\n",
    "    \n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bba1e2-a806-4200-b0c9-575ae2c973c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def calculate_precision_recall(model, dataloader):\n",
    "    model.eval()\n",
    "    true_colors = []\n",
    "    pred_colors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for L, ab in dataloader:\n",
    "            L, ab = L.to(device), ab.to(device)\n",
    "            outputs = model(L)\n",
    "            \n",
    "            # Convert to color bins\n",
    "            true_bins = (ab * 4 + 4).clamp(0, 7).long().flatten()\n",
    "            pred_bins = (outputs * 4 + 4).clamp(0, 7).long().flatten()\n",
    "            \n",
    "            true_colors.extend(true_bins.cpu().numpy())\n",
    "            pred_colors.extend(pred_bins.cpu().numpy())\n",
    "    \n",
    "    precision = precision_score(true_colors, pred_colors, average='macro', zero_division=0)\n",
    "    recall = recall_score(true_colors, pred_colors, average='macro', zero_division=0)\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dccc40d2-eb04-41d1-810e-e2cb0279a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_image(input_image):\n",
    "    # Convert to LAB\n",
    "    img_np = np.array(input_image) / 255.0\n",
    "    lab_img = rgb2lab(img_np).astype(\"float32\")\n",
    "    L = lab_img[:, :, 0] / 100.0\n",
    "    \n",
    "    # Convert to tensor\n",
    "    L_tensor = torch.tensor(L).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        ab_pred = model(L_tensor)\n",
    "    \n",
    "    # Convert back to RGB\n",
    "    ab_pred = ab_pred.squeeze().cpu().numpy() * 128\n",
    "    pred_lab = np.concatenate([\n",
    "        (lab_img[:, :, 0]),\n",
    "        ab_pred.transpose(1, 2, 0)\n",
    "    ], axis=2)\n",
    "    pred_rgb = (lab2rgb(pred_lab) * 255).astype('uint8')\n",
    "    \n",
    "    return pred_rgb\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e82bff-2476-46cc-92d3-0f4f34dc7ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    train_model(epochs=10)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    avg_psnr, avg_ssim = evaluate_model(model, test_loader)\n",
    "    \n",
    "    # Confusion Matrix and Metrics\n",
    "    print(\"\\nCalculating confusion matrix...\")\n",
    "    color_confusion_matrix(model, test_loader)\n",
    "    \n",
    "    print(\"\\nCalculating precision/recall...\")\n",
    "    precision, recall = calculate_precision_recall(model, test_loader)\n",
    "    \n",
    "    # Save model if metrics are good\n",
    "    if avg_psnr > 20 and avg_ssim > 0.7 and precision > 0.7:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'metrics': {\n",
    "                'psnr': avg_psnr,\n",
    "                'ssim': avg_ssim,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "        }, \"final_colorization_model.pth\")\n",
    "        print(\"\\nModel saved with good performance!\")\n",
    "    \n",
    "    # Launch GUI with proper examples\n",
    "    print(\"\\nLaunching GUI...\")\n",
    "    examples = [\n",
    "        [\"examples/grayscale1.jpg\"],\n",
    "        [\"examples/grayscale2.jpg\"]\n",
    "    ]\n",
    "    \n",
    "    iface = gr.Interface(\n",
    "        fn=colorize_image,\n",
    "        inputs=gr.Image(type=\"pil\"),\n",
    "        outputs=\"image\",\n",
    "        title=\"Image Colorization\",\n",
    "        examples=examples\n",
    "    )\n",
    "    iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b4bce-d0a1-45e2-a348-21ddb804c9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01bcf22-e6fa-4665-b0e8-ce7041259159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde3710-27d7-4249-8051-2ccc21dc992c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed62bc-2c23-4a9c-85aa-ec9bf254fbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc522c15-47d7-4a55-8689-77be6bc8b090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
